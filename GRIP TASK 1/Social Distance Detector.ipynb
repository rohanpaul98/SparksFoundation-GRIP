{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Importing the Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import distance as dist\n",
    "import numpy as np #pip install numpy\n",
    "import imutils #pip install imutils\n",
    "import cv2 #pip install opencv-python\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Importing YOLOv3 configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base path to YOLO directory \n",
    "MODEL_PATH = \"yolo-coco\"\n",
    "\n",
    "MIN_CONF = 0.3\n",
    "NMS_THRESH = 0.3\n",
    "\n",
    "#boolean indication if NVIDIA CUDA GPU should be used\n",
    "USE_GPU = False\n",
    "\n",
    "#define the minimum safe distance (in pixels) that two people can be from each other.\n",
    "MIN_DISTANCE = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['person', 'bicycle', 'car', 'motorbike', 'aeroplane', 'bus', 'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket', 'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair', 'sofa', 'pottedplant', 'bed', 'diningtable', 'toilet', 'tvmonitor', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush']\n",
      "80\n",
      "loading YOLO from disk\n"
     ]
    }
   ],
   "source": [
    "#load the coco class labels our YOLO was trained on\n",
    "labelsPath = os.path.sep.join([MODEL_PATH,\"coco.names\"])\n",
    "LABELS = open(labelsPath).read().strip().split(\"\\n\")\n",
    "\n",
    "print(LABELS)\n",
    "\n",
    "print(len(LABELS))\n",
    "\n",
    "#derive the paths to the YOLO weights and model configuration\n",
    "weightsPath = os.path.sep.join([MODEL_PATH,\"yolov3.weights\"])\n",
    "configPath = os.path.sep.join([MODEL_PATH,\"yolov3.cfg\"])\n",
    "\n",
    "#load our YOLO object detector trained on Coco dataset(80 classes)\n",
    "print(\"loading YOLO from disk\")\n",
    "net = cv2.dnn.readNetFromDarknet(configPath,weightsPath)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Using GPU for preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking whether to use GPU or not\n",
    "if USE_GPU:\n",
    "    #setting CUDA as the preferable backend and target \n",
    "    print(\"Setting prefferable backend and target to CUDA...\")\n",
    "    net.setPreferableBackend(cv2.dnn.DNN_BACKEND_CUDA)\n",
    "    net.setPreferableTarget(cv2.dnn.DNN_TARGET_CUDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine only the \"output\" layer names that we need from YOLO\n",
    "ln = net.getLayerNames()\n",
    "ln = [ln[i[0]-1] for i in net.getUnconnectedOutLayers()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4: Input access(Video File and Live Footage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accesing Video stream\n",
      "FPS of current video frame -  25.0\n",
      "Number of frames in the video: 531.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Accesing Video stream\")\n",
    "#upload the video file you want to check the social distancing\n",
    "vs = cv2.VideoCapture(\"2.mp4\")\n",
    "\n",
    "#Number of frames per second\n",
    "fps = vs.get(cv2.CAP_PROP_FPS)\n",
    "print(\"FPS of current video frame - \",fps)\n",
    "\n",
    "num_frames = vs.get(cv2.CAP_PROP_FRAME_COUNT)\n",
    "print(\"Number of frames in the video:\",num_frames)\n",
    "\n",
    "writer= None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithm Detecting People"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_people(frame,net,ln,personIdx=0):\n",
    "    # grab the dimensions of the frame and initialize the list of results\n",
    "    (H,W) = frame.shape[:2];\n",
    "    results = []\n",
    "    \n",
    "    #constructing a blob from the input frame and then pass a forward pass of the YOLO object detector,giving us our bounding boxes\n",
    "    #and the associated probabilities.\n",
    "    blob = cv2.dnn.blobFromImage(frame,1/255.0,(416,416),swapRB=True,crop=False)\n",
    "    net.setInput(blob)\n",
    "    layerOutputs = net.forward(ln)\n",
    "    \n",
    "    \n",
    "    # Initialize our lists of detected bounding boxes,centroids and confidences respectively. \n",
    "    boxes=[]\n",
    "    centroids=[]\n",
    "    confidences=[]\n",
    "    \n",
    "    #looping over each of the layer outputs\n",
    "    for output in layerOutputs:\n",
    "        #loop each of the detections\n",
    "        for detection in output:\n",
    "            #extract the class id and confidences(ie.,probabibility) of the current object detection\n",
    "            scores = detection[5:]\n",
    "            classID = np.argmax(scores)\n",
    "            confidence = scores[classID]\n",
    "            \n",
    "            #filter detections by (1) ensuring that the object detected was a person and \n",
    "            #(2)that the minimum confidence is net \n",
    "            if classID == personIdx and confidence >MIN_CONF:\n",
    "                #scale the bounding box coordinates back relative to the size of the image,keeping in mind the YOLO \n",
    "                #actually returns the center (x,y)- coordinates of the bounding box  followed by the boxes width and height.\n",
    "                box = detection[0:4]*np.array([W,H,W,H])\n",
    "                (centerX,centerY,width,height) = box.astype(\"int\")\n",
    "                \n",
    "                #use the center(x,y) coordinates to derive the top and left corner of the bounding boxes\n",
    "                x = int(centerX-(width/2))\n",
    "                y = int(centerY-(height/2))\n",
    "                \n",
    "                #update our list of bounding box coordinates, centroids and confidences\n",
    "                boxes.append([x,y,int(width),int(height)])\n",
    "                centroids.append((centerX,centerY))\n",
    "                confidences.append(float(confidence))\n",
    "    #apply non-maxima suppresion to suppress weak, overlapping bounding boxes \n",
    "    idxs = cv2.dnn.NMSBoxes(boxes,confidences,MIN_CONF,NMS_THRESH)\n",
    "    \n",
    "    #ensure that at least one detection exist\n",
    "    if len(idxs)>0:\n",
    "        #loop over the indices we are keeping \n",
    "        for i in idxs.flatten():\n",
    "            #extract the bounding box coordinates\n",
    "            (x,y) = (boxes[i][0],boxes[i][1])\n",
    "            (w,h) = (boxes[i][2],boxes[i][3])\n",
    "            \n",
    "            #update our results list to consist of the person prediction probability bounding box coordiantes and the centroid.\n",
    "            r = (confidences[i],(x,y,x+w,y+h),centroids[i])\n",
    "            results.append(r)\n",
    "    #return the list of results\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "display = 1\n",
    "output = \"Output.avi\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    #read the next frame from the file\n",
    "    (grabbed,frame ) = vs.read()\n",
    "    \n",
    "    #if the frame is not grabbed then we can say that we have reached the end of the stream\n",
    "    if not grabbed:\n",
    "        break\n",
    "    #resize the frame and detect people in it\n",
    "    frame = imutils.resize(frame,width=700)\n",
    "    results = detect_people(frame,net,ln,personIdx=LABELS.index(\"person\"))\n",
    "    \n",
    "    #initialize the set of indexes that violate social distance\n",
    "    violate = set()\n",
    "    if len(results) >=2:\n",
    "        #extract all the centroids from the results and compute the euclidean distances between all the pairs of centroids\n",
    "        centroids = np.array([r[2] for r in results])\n",
    "        D = dist.cdist(centroids,centroids,metric=\"euclidean\")\n",
    "        \n",
    "        #loop over the upper traiangular of the distance matrix\n",
    "        for i in range(0,D.shape[0]):\n",
    "            for j in range(i+1,D.shape[1]):\n",
    "                #check to see if the distance between any two centroid pairs is less than the configured number of pixels\n",
    "                if D[i,j] <MIN_DISTANCE:\n",
    "                    #update our violations set with the indexes of the centroid pairs \n",
    "                    violate.add(i)\n",
    "                    violate.add(j)\n",
    "                    \n",
    "    #loop over the results\n",
    "    for (i,(prob,bbox,centroid)) in enumerate(results):\n",
    "        #extract the bounding boxes and centroid coordinates,then initialize the color of annotation\n",
    "        (startX,startY,endX,endY) = bbox\n",
    "        (cX,cY) = centroid\n",
    "        color = (0,255,0)\n",
    "        \n",
    "        #if the index pair exists within the violation set then update the color\n",
    "        if i in violate:\n",
    "            color=(0,0,255)\n",
    "        \n",
    "        #draw (1) a bounding box around the person and\n",
    "        #(2) the centroid coordinates of the person\n",
    "        cv2.rectangle(frame,(startX,startY),(endX,endY),color,2)\n",
    "        cv2.circle(frame,(cX,cY),5,color,1)\n",
    "    \n",
    "    #display the total number of violations on the output frame\n",
    "    text = \"Social distance violations:{}\".format(len(violate))\n",
    "    cv2.putText(frame,text,(10,frame.shape[0]-25),cv2.FONT_HERSHEY_SIMPLEX,0.85,(0,0,255),3)\n",
    "    \n",
    "    #check to see if the output frame is to be displayed or not\n",
    "    if display > 0:\n",
    "        #show the output frame\n",
    "        cv2.imshow(\"Frame\",frame)\n",
    "        key = cv2.waitKey(1) & 0xFF\n",
    "        \n",
    "        #if the key 'q' is pressed it will end the frame\n",
    "        if key == ord('q'):\n",
    "            break\n",
    "    \n",
    "    #if an output video file has been passed and the video writer has not been initialised then\n",
    "    if output !=\"\" and writer is None:\n",
    "        #initializing our video writer\n",
    "        fourcc = cv2.VideoWriter_fourcc(*\"MJPG\")\n",
    "        writer = cv2.VideoWriter(output,fourcc,25,(frame.shape[1],frame.shape[0]),True)\n",
    "    #if the video writer is not none,write the frame to the output\n",
    "    #video file\n",
    "    if writer is not None:\n",
    "        writer.write(frame)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
